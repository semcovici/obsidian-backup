Spark é uma ferramenta de software livre para processar grandes conjuntos de dados, principalmente para trabalhar com Big Data. Ele foi projetado para fornecer velocidade computacional, escalabilidade e programabilidade necessárias para Big Data, especificamente para aplicativos de streaming de dados, dados de gráficos, machine learning e inteligência artificial (IA).

Uma única máquina não é poderosa o suficiente para computar em grandes conjuntos de dados em um tempo hábil, por isso usamos um [[Cluster]] para rodar aplicações de Big Data.

![[images/Pasted image 20240412152518.png]]

O apache Spark trabalha junto com o Cluster Manager para distribuir os recursos de dados e computação entre as máquinas disponíveis no cluster.

Existem APIs para diversas linguagens, como Scala, Python, Java, R e C# .NET.

### Life Cycle de uma aplicação 

![[images/Pasted image 20240412153340.png]]





